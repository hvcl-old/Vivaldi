--------------------------------------------------------------------------
An error occurred while trying to map in the address of a function.
  Function Name: cuPointerSetAttribute
  Error string:  /usr/lib64/libcuda.so.1: undefined symbol: cuPointerSetAttribute
CUDA-aware support is disabled.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              head
  Registerable memory:     2048 MiB
  Total memory:            49139 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
An error occurred while trying to map in the address of a function.
  Function Name: cuPointerSetAttribute
  Error string:  /usr/lib64/libcuda.so.1: undefined symbol: cuPointerSetAttribute
CUDA-aware support is disabled.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: It appears that your OpenFabrics subsystem is configured to only
allow registering part of your physical memory.  This can cause MPI jobs to
run with erratic performance, hang, and/or crash.

This may be caused by your OpenFabrics vendor limiting the amount of
physical memory that can be registered.  You should investigate the
relevant Linux kernel module parameters that control how much physical
memory can be registered, and increase them to allow registering all
physical memory on your machine.

See this Open MPI FAQ item for more information on these Linux kernel module
parameters:

    http://www.open-mpi.org/faq/?category=openfabrics#ib-locked-pages

  Local host:              head
  Registerable memory:     2048 MiB
  Total memory:            49139 MiB

Your MPI job will continue, but may be behave poorly and/or hang.
--------------------------------------------------------------------------
Scheduler wait
Scheduler source: 0 flag: update_computing_unit
Scheduler wait
Reader waiting
Scheduler source: 4 flag: set_function
Scheduler wait
+++ Loading image ...
+++ Loading Done!
+++ Input & output split test +++
Scheduler source: 4 flag: retain
Scheduler wait
Scheduler source: 2 flag: notice
Scheduler wait
Scheduler source: 2 flag: idle
Scheduler wait
Reader source: 4 flag: recv
Reader waiting
Scheduler source: 4 flag: function
FLAG identical
Scheduler wait
Scheduler source: 4 flag: retain
Scheduler wait
Reader source: 1 flag: memcpy_p2p_send
Scheduler source: 2 flag: idle
Reader waiting
Scheduler wait
Reader source: 1 flag: memcpy_p2p_send
Scheduler source: 2 flag: idle
Scheduler wait
Reader waiting
Scheduler source: 4 flag: function
FLAG identical
Scheduler wait
Scheduler source: 4 flag: retain
Scheduler wait
Scheduler source: 4 flag: synchronize
Scheduler wait
GPU: 3 waiting
GPU: 3 source: 2 flag: memcpy_p2p_recv
Scheduler source: 3 flag: notice
GPU: 3 waiting
GPU: 3 source: 1 flag: set_function
Scheduler wait
Scheduler source: 3 flag: release
Scheduler wait
Scheduler source: 3 flag: idle
Scheduler wait
Scheduler source: 3 flag: notice
GPU: 3 waiting
GPU: 3 source: 2 flag: memcpy_p2p_recv
GPU: 3 waiting
GPU: 3 source: 1 flag: run_function
Scheduler wait
Scheduler source: 3 flag: release
Scheduler wait
Scheduler source: 3 flag: idle
Scheduler wait
PRINT DEVPTR Data package
--------------------------------------------
unique_id*                         0
split_shape*                       {'y': 2, 'x': 1, 'z': 1, 'w': 1}
split_position*                    {'y': 1, 'x': 1, 'z': 1, 'w': 1}
data_range                         {'y': (0, 6), 'x': (0, 10)}
data_halo                          1
data_shape                         (6, 10)
data_memory_shape                  (6, 10)
data_dtype                         <type 'numpy.ndarray'>
data_contents_dtype                float
data_contents_memory_dtype         <type 'numpy.float32'>
data_contents_memory_shape         [1]
data_bytes                         240
full_data_range                    {'y': (0, 10), 'x': (0, 10)}
full_data_shape                    [10, 10]
full_data_memory_shape             (10, 10)
full_data_bytes                    400
usage                              240
out_of_core                        False
--------------------------------------------
Scheduler source: 3 flag: release
[[ 200.  200.  200.  200.  200.  200.  200.  200.  200.  200.]
 [ 200.  200.  200.  200.  200.  200.  200.  200.  200.  200.]
 [ 200.  200.  200.  200.  200.  200.  200.  200.  200.  200.]
 [ 200.  200.  200.  200.  200.  200.  200.  200.  200.  200.]
 [ 200.  200.  200.  200.  200.  200.  200.  200.  200.  200.]
 [ 100.  100.  100.  100.  100.  100.  100.  100.  100.  100.]]
Scheduler wait
Scheduler source: 3 flag: notice
Scheduler wait
Scheduler source: 3 flag: idle
Scheduler wait
GPU: 3 waiting
GPU: 3 source: 1 flag: run_function
PRINT DEVPTR Data package
--------------------------------------------
unique_id*                         0
split_shape*                       {'y': 2, 'x': 1, 'z': 1, 'w': 1}
split_position*                    {'y': 2, 'x': 1, 'z': 1, 'w': 1}
data_range                         {'y': (4, 10), 'x': (0, 10)}
data_halo                          1
data_shape                         (6, 10)
data_memory_shape                  (6, 10)
data_dtype                         <type 'numpy.ndarray'>
data_contents_dtype                float
data_contents_memory_dtype         <type 'numpy.float32'>
data_contents_memory_shape         [1]
data_bytes                         240
full_data_range                    {'y': (0, 10), 'x': (0, 10)}
full_data_shape                    [10, 10]
full_data_memory_shape             (10, 10)
full_data_bytes                    400
usage                              240
out_of_core                        False
--------------------------------------------
Scheduler source: 3 flag: release
Scheduler wait
Scheduler source: 3 flag: notice
[[ 200.  200.  200.  200.  200.  200.  200.  200.  200.  200.]
 [ 100.  100.  100.  100.  100.  100.  100.  100.  100.  100.]
 [ 100.  100.  100.  100.  100.  100.  100.  100.  100.  100.]
 [ 100.  100.  100.  100.  100.  100.  100.  100.  100.  100.]
 [ 100.  100.  100.  100.  100.  100.  100.  100.  100.  100.]
 [ 100.  100.  100.  100.  100.  100.  100.  100.  100.  100.]]
GPU: 3 waiting
GPU: 3 source: 1 flag: memcpy_p2p_send
p2p_send {'y': (0, 5), 'x': (0, 10)} {'y': (0, 6), 'x': (0, 10)} {'y': (0, 5), 'x': (0, 10)} 3 3 True
GPU: 3 waiting
GPU: 3 source: 1 flag: memcpy_p2p_send
p2p_send {'y': (5, 10), 'x': (0, 10)} {'y': (0, 6), 'x': (0, 10)} {'y': (5, 6), 'x': (0, 10)} 3 3 True
GPU: 3 waiting
Scheduler wait
Scheduler source: 3 flag: idle
Scheduler wait
Scheduler source: 3 flag: notice
Scheduler wait
Scheduler source: 3 flag: idle
Scheduler wait
Scheduler source: 3 flag: notice
GPU: 3 source: 1 flag: memcpy_p2p_send
p2p_send {'y': (0, 6), 'x': (0, 10)} {'y': (4, 10), 'x': (0, 10)} {'y': (4, 5), 'x': (0, 10)} 3 3 True
GPU: 3 waiting
GPU: 3 source: 1 flag: memcpy_p2p_send
p2p_send {'y': (4, 10), 'x': (0, 10)} {'y': (4, 10), 'x': (0, 10)} {'y': (5, 10), 'x': (0, 10)} 3 3 True
GPU: 3 waiting
Scheduler wait
Scheduler source: 3 flag: idle
Scheduler wait
Scheduler source: 3 flag: notice
Scheduler wait
Scheduler source: 3 flag: idle
Scheduler wait
Scheduler source: 3 flag: notice
GPU: 3 source: 1 flag: run_function
Scheduler wait
Scheduler source: 3 flag: idle
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
GPU: 3 waiting
GPU: 3 source: 1 flag: run_function
Reader source: 1 flag: synchronize
Reader waiting
+++ Done! +++
   
Scheduler wait
Scheduler source: 3 flag: release
Scheduler wait
Scheduler source: 3 flag: release
Scheduler wait
Scheduler source: 3 flag: release
Scheduler wait
Scheduler source: 3 flag: notice
Scheduler wait
Scheduler source: 3 flag: idle
Scheduler wait
Scheduler source: 3 flag: release
Scheduler wait
Scheduler source: 3 flag: release
Scheduler wait
Scheduler source: 3 flag: release
Scheduler wait
Scheduler source: 3 flag: notice
Scheduler wait
Scheduler source: 3 flag: idle
Scheduler wait
Scheduler source: 4 flag: gather
GPU: 3 waiting
GPU: 3 source: 1 flag: synchronize
GPU: 3 waiting
Scheduler wait
Scheduler source: 3 flag: notice
Scheduler wait
Scheduler source: 3 flag: idle
Scheduler wait
Scheduler source: 3 flag: notice
GPU: 3 source: 1 flag: memcpy_p2p_send
p2p_send {'y': (0, 5), 'x': (0, 10)} {'y': (0, 10), 'x': (0, 10)} {'y': (0, 5), 'x': (0, 10)} 3 3 True
GPU: 3 waiting
GPU: 3 source: 1 flag: memcpy_p2p_send
p2p_send {'y': (5, 10), 'x': (0, 10)} {'y': (0, 10), 'x': (0, 10)} {'y': (5, 10), 'x': (0, 10)} 3 3 True
Scheduler wait
Scheduler source: 3 flag: idle
Scheduler wait
GPU: 3 waiting
GPU: 3 source: 1 flag: memcpy_p2p_send
Scheduler source: 3 flag: release
Scheduler wait
PRINT DEVPTR Data package
--------------------------------------------
unique_id*                         2
split_shape*                       {'y': 1, 'x': 1, 'z': 1, 'w': 1}
split_position*                    {'y': 1, 'x': 1, 'z': 1, 'w': 1}
data_range                         {'y': (0, 10), 'x': (0, 10)}
data_halo                          0
data_shape                         (10, 10)
data_memory_shape                  (10, 10)
data_dtype                         <type 'numpy.ndarray'>
data_contents_dtype                float
data_contents_memory_dtype         <type 'numpy.float32'>
data_contents_memory_shape         [1]
data_bytes                         400
full_data_range                    {'y': (0, 10), 'x': (0, 10)}
full_data_shape                    (10, 10)
full_data_memory_shape             (10, 10)
full_data_bytes                    400
usage                              400
out_of_core                        False
--------------------------------------------
Scheduler source: 3 flag: release
Scheduler wait
[[ 200.  200.  200.  200.  200.  200.  200.  200.  200.  200.]
 [ 200.  200.  200.  200.  200.  200.  200.  200.  200.  200.]
 [ 200.  200.  200.  200.  200.  200.  200.  200.  200.  200.]
 [ 200.  200.  200.  200.  200.  200.  200.  200.  200.  200.]
 [ 200.  200.  200.  200.  200.  200.  200.  200.  200.  200.]
 [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]
 [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]
 [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]
 [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]
 [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]]
PRINT DEVPTR Data package
--------------------------------------------
unique_id*                         2
split_shape*                       {'y': 1, 'x': 1, 'z': 1, 'w': 1}
split_position*                    {'y': 1, 'x': 1, 'z': 1, 'w': 1}
data_range                         {'y': (0, 10), 'x': (0, 10)}
data_halo                          0
data_shape                         (10, 10)
data_memory_shape                  (10, 10)
data_dtype                         <type 'numpy.ndarray'>
data_contents_dtype                float
data_contents_memory_dtype         <type 'numpy.float32'>
data_contents_memory_shape         [1]
data_bytes                         400
full_data_range                    {'y': (0, 10), 'x': (0, 10)}
full_data_shape                    (10, 10)
full_data_memory_shape             (10, 10)
full_data_bytes                    400
usage                              400
out_of_core                        False
--------------------------------------------
[[ 200.  200.  200.  200.  200.  200.  200.  200.  200.  200.]
 [ 200.  200.  200.  200.  200.  200.  200.  200.  200.  200.]
 [ 200.  200.  200.  200.  200.  200.  200.  200.  200.  200.]
 [ 200.  200.  200.  200.  200.  200.  200.  200.  200.  200.]
 [ 200.  200.  200.  200.  200.  200.  200.  200.  200.  200.]
 [ 100.  100.  100.  100.  100.  100.  100.  100.  100.  100.]
 [ 100.  100.  100.  100.  100.  100.  100.  100.  100.  100.]
 [ 100.  100.  100.  100.  100.  100.  100.  100.  100.  100.]
 [ 100.  100.  100.  100.  100.  100.  100.  100.  100.  100.]
 [ 100.  100.  100.  100.  100.  100.  100.  100.  100.  100.]]
p2p_send {'y': (0, 10), 'x': (0, 10)} {'y': (0, 10), 'x': (0, 10)} {'y': (0, 10), 'x': (0, 10)} 3 4 False
Scheduler source: 3 flag: idle
Scheduler wait
GPU: 3 waiting
GPU: 3 source: 1 flag: free
GPU: 3 waiting
GPU: 3 source: 1 flag: free
GPU: 3 waiting
[[ 200.  200.  200.  200.  200.  200.  200.  200.  200.  200.]
 [ 200.  200.  200.  200.  200.  200.  200.  200.  200.  200.]
 [ 200.  200.  200.  200.  200.  200.  200.  200.  200.  200.]
 [ 200.  200.  200.  200.  200.  200.  200.  200.  200.  200.]
 [ 200.  200.  200.  200.  200.  200.  200.  200.  200.  200.]
 [ 100.  100.  100.  100.  100.  100.  100.  100.  100.  100.]
 [ 100.  100.  100.  100.  100.  100.  100.  100.  100.  100.]
 [ 100.  100.  100.  100.  100.  100.  100.  100.  100.  100.]
 [ 100.  100.  100.  100.  100.  100.  100.  100.  100.  100.]
 [ 100.  100.  100.  100.  100.  100.  100.  100.  100.  100.]]
+++ Save image +++
Scheduler source: 4 flag: synchronize
Scheduler wait
+++ Done! +++
Vivaldi execution time: 13183 msec
